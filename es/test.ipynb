{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2020 Los autores de TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
        },
        "colab_type": "code",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [

      ],
      "source": [

      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qFdPvlXBOdUN"
      },
      "source": [
        "# Introducción a los gradientes y la diferenciación automática"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/guide/autodiff\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">View on TensorFlow.org</a>   </td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/autodiff.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Run in Google Colab</a>   </td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/guide/autodiff.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">View source on GitHub</a>   </td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/guide/autodiff.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Download notebook</a>   </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r6P32iYYV27b"
      },
      "source": [
        "## Automatic Differentiation and Gradients\n",
        "\n",
        "[Automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) is useful for implementing machine learning algorithms such as [backpropagation](https://en.wikipedia.org/wiki/Backpropagation) for training neural networks.\n",
        "\n",
        "In this guide, we will discuss ways you can compute gradients with TensorFlow, especially in eager execution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MUXex9ctTuDB"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "IqR2PQG4ZaZ0"
      },
      "outputs": [

      ],
      "source": [

      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xHxb-dlhMIzW"
      },
      "source": [
        "## Computing gradients\n",
        "\n",
        "Para diferenciarse automáticamente, TensorFlow necesita recordar qué operaciones ocurren y en qué orden durante el pase *hacia adelante* . Luego, durante el *paso hacia atrás* , TensorFlow recorre esta lista de operaciones en orden inverso para calcular los gradientes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1CLWJl0QliB0"
      },
      "source": [
        "## Gradient tapes\n",
        "\n",
        "TensorFlow provides the [tf.GradientTape](https://www.tensorflow.org/api_docs/python/tf/GradientTape) API for automatic differentiation; that is, computing the gradient of a computation with respect to some inputs, usually `tf.Variable`s. TensorFlow \"records\" relevant operations executed inside the context of a `tf.GradientTape` onto a \"tape\". TensorFlow then uses that tape to compute the gradients of a \"recorded\" computation using [reverse mode differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation).\n",
        "\n",
        "Here is a simple example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "Xq9GgTCP7a4A"
      },
      "outputs": [

      ],
      "source": [

      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CR9tFAP_7cra"
      },
      "source": [
        "Once you've recorded some operations, use `GradientTape.gradient(target, sources)` to calculate the gradient of some target (often a loss) relative to some source (often the model's variables)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "LsvrwF6bHroC"
      },
      "outputs": [

      ],
      "source": [

      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Q2_aqsO25Vx1"
      },
      "source": [
        "The above example uses scalars, but `tf.GradientTape` works as easily on any tensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "vacZ3-Ws5VdV"
      },
      "outputs": [

      ],
      "source": [

      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i4eXOkrQ-9Pb"
      },
      "source": [
        "To get the gradient of `y` with respect to both variables, you can pass both as sources to the `gradient` method. The tape is flexible about how sources are passed and will accept any nested combination of lists or dictionaries and return the gradient structured the same way (see `tf.nest`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "luOtK1Da_BR0"
      },
      "outputs": [

      ],
      "source": [

      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ei4iVXi6qgM7"
      },
      "source": [
        "The gradient with respect to each source has the shape of the source:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "aYbWRFPZqk4U"
      },
      "outputs": [

      ],
      "source": [

      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dI_SzxHsvao1"
      },
      "source": [
        "Here is the gradient calculation again, this time passing a dictionary of variables:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "d73cY6NOuaMd"
      },
      "outputs": [

      ],
      "source": [

      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HZ2LvHifEMgO"
      },
      "source": [
        "## Gradients with respect to a model\n",
        "\n",
        "It's common to collect `tf.Variables` into a `tf.Module` or one of its subclasses (`layers.Layer`, `keras.Model`) for [checkpointing](checkpoint.ipynb) and [exporting](saved_model.ipynb).\n",
        "\n",
        "In most cases, you will want to calculate gradients with respect to a model's trainable variables.  Since all subclasses of `tf.Module` aggregate their variables in the `Module.trainable_variables` property, you can calculate these gradients in a few lines of code: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "JvesHtbQESc-"
      },
      "outputs": [

      ],
      "source": [

      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "PR_ezr6UFrpI"
      },
      "outputs": [

      ],
      "source": [

      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f6Gx6LS714zR"
      },
      "source": [
        "<a id=\"watches\"></a>\n",
        "\n",
        "## Controlling what the tape watches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "N4VlqKFzzGaC"
      },
      "source": [
        "The default behavior is to record all operations after accessing a trainable `tf.Variable`. The reasons for this are:\n",
        "\n",
        "- The tape needs to know which operations to record in the forward pass to calculate the gradients in the backwards pass.\n",
        "- The tape holds references to intermediate outputs, so you don't want to record unnecessary operations.\n",
        "- The most common use case involves calculating the gradient of a loss with respect to all a model's trainable variables.\n",
        "\n",
        "For example the following fails to calculate a gradient because the `tf.Tensor` is not \"watched\" by default, and the `tf.Variable` is not trainable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "Kj9gPckdB37a"
      },
      "outputs": [

      ],
      "source": [

      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RkcpQnLgNxgi"
      },
      "source": [
        "You can list the variables being watched by the tape using the `GradientTape.watched_variables` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "hwNwjW1eAkib"
      },
      "outputs": [

      ],
      "source": [

      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NB9I1uFvB4tf"
      },
      "source": [
        "`tf.GradientTape` provides hooks that give the user control over what is or is not watched.\n",
        "\n",
        "To record gradients with respect to a `tf.Tensor`, you need to call `GradientTape.watch(x)`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "tVN1QqFRDHBK"
      },
      "outputs": [

      ],
      "source": [

      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qxsiYnf2DN8K"
      },
      "source": [
        "Conversely, to disable the default behavior of watching all `tf.Variables`, set `watch_accessed_variables=False` when creating the gradient tape. This calculation uses two variables, but only connects the gradient for one of the variables:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "7QPzwWvSEwIp"
      },
      "outputs": [

      ],
      "source": [

      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TRduLbE1H2IJ"
      },
      "source": [
        "Since `GradientTape.watch` was not called on `x0`, no gradient is computed with respect to it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "e6GM-3evH1Sz"
      },
      "outputs": [

      ],
      "source": [

      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2g1nKB6P-OnA"
      },
      "source": [
        "## Intermediate results\n",
        "\n",
        "You can also request gradients of the output with respect to intermediate values computed inside the `tf.GradientTape` context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "7XaPRAwUyYms"
      },
      "outputs": [

      ],
      "source": [

      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ISkXuY7YzIcS"
      },
      "source": [
        "By default, the resources held by a `GradientTape` are released as soon as `GradientTape.gradient()` method is called. To compute multiple gradients over the same computation, create a `persistent` gradient tape. This allows multiple calls to the `gradient()` method as resources are released when the tape object is garbage collected. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "zZaCm3-9zVCi"
      },
      "outputs": [

      ],
      "source": [

      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "j8bv_jQFg6CN"
      },
      "outputs": [

      ],
      "source": [

      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O_ZY-9BUB7vX"
      },
      "source": [
        "## Notes on performance\n",
        "\n",
        "- There is a tiny overhead associated with doing operations inside a gradient tape context. For most eager execution this will not be a noticeable cost, but you should still use tape context around the areas only where it is required.\n",
        "\n",
        "- Gradient tapes use memory to store intermediate results, including inputs and outputs, for use during the backwards pass.\n",
        "\n",
        "    For efficiency, some ops (like `ReLU`) don't need to keep their intermediate results and they are pruned during the forward pass. However, if you use `persistent=True` on your tape, *nothing is discarded* and your peak memory usage will be higher."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9dLBpZsJebFq"
      },
      "source": [
        "## Gradients of non-scalar targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7pldU9F5duP2"
      },
      "source": [
        "A gradient is fundamentally an operation on a scalar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "qI0sDV_WeXBb"
      },
      "outputs": [

      ],
      "source": [

      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "COEyYp34fxj4"
      },
      "source": [
        "Thus, if you ask for the gradient of multiple targets, the result for each source is:\n",
        "\n",
        "- The gradient of the sum of the targets, or equivalently\n",
        "- The sum of the gradients of each target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "o4a6_YOcfWKS"
      },
      "outputs": [

      ],
      "source": [

      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uvP-mkBMgbym"
      },
      "source": [
        "Similarly, if the target(s) are not scalar the gradient of the sum is calculated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "DArPWqsSh5un"
      },
      "outputs": [

      ],
      "source": [

      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "flDbx68Zh5Lb"
      },
      "source": [
        "This makes it simple to take the gradient of the sum of a collection of losses, or the gradient of the sum of an element-wise loss calculation.\n",
        "\n",
        "If you need a separate gradient for each item see [Jacobians](advanced_autodiff.ipynb#jacobians)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iwFswok8RAly"
      },
      "source": [
        "In some cases you can skip the Jacobian. For an element-wise calculation, the gradient of the sum gives the derivative of each element with respect to its input-element, since each element is independent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "JQvk_jnMmTDS"
      },
      "outputs": [

      ],
      "source": [

      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "e_f2QgDPmcPE"
      },
      "outputs": [

      ],
      "source": [

      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6kADybtQzYj4"
      },
      "source": [
        "## Control flow\n",
        "\n",
        "Because tapes record operations as they are executed, Python control flow (using `if`s and `while`s for example) is naturally handled.\n",
        "\n",
        "Here a different variable is used on each branch of an `if`. The gradient only connects to the variable that was used:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "ciFLizhrrjy7"
      },
      "outputs": [

      ],
      "source": [

      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HKnLaiapsjeP"
      },
      "source": [
        "Just remember that the control statements themselves are not differentiable, so they are invisible to gradient-based optimizers.\n",
        "\n",
        "Depending on the value of `x` in the above example, the tape either records `result = v0` or `result = v1**2`. The gradient with respect to `x` is always `None`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "8k05WmuAwPm7"
      },
      "outputs": [

      ],
      "source": [

      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "egypBxISAHhx"
      },
      "source": [
        "## Getting a gradient of `None`\n",
        "\n",
        "When a target is not connected to a source you will get a gradient of `None`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "CU185WDM81Ut"
      },
      "outputs": [

      ],
      "source": [

      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sZbKpHfBRJym"
      },
      "source": [
        "Here `z` is obviously not connected to `x`, but there are several less-obvious ways that a gradient can be disconnected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eHDzDOiQ8xmw"
      },
      "source": [
        "### 1. Replaced a variable with a tensor.\n",
        "\n",
        "In the section on [\"controlling what the tape watches\"](#watches) you saw that the tape will automatically watch a `tf.Variable` but not a `tf.Tensor`.\n",
        "\n",
        "One common error is to inadvertently replace a `tf.Variable` with a `tf.Tensor`, instead of using `Variable.assign` to update the `tf.Variable`. Here is an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "QPKY4Tn9zX7_"
      },
      "outputs": [

      ],
      "source": [

      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3gwZKxgA97an"
      },
      "source": [
        "### 2. Did calculations outside of TensorFlow\n",
        "\n",
        "The tape can't record the gradient path if the calculation exits TensorFlow. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "jmoLCDJb_yw1"
      },
      "outputs": [

      ],
      "source": [

      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "p3YVfP3R-tp7"
      },
      "source": [
        "### 3. Took gradients through an integer or string\n",
        "\n",
        "Integers and strings are not differentiable. If a calculation path uses these data types there will be no gradient.\n",
        "\n",
        "Nobody expects strings to be differentiable, but it's easy to accidentally create an `int` constant or variable if you don't specify the `dtype`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "9jlHXHqfASU3"
      },
      "outputs": [

      ],
      "source": [

      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RsdP_mTHX9L1"
      },
      "source": [
        "TensorFlow doesn't automatically cast between types, so in practice you'll often get a type error instead of a missing gradient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WyAZ7C8qCEs6"
      },
      "source": [
        "### 5. Took gradients through a stateful object\n",
        "\n",
        "State stops gradients. When you read from a stateful object the tape can only see the current state, not the history that lead to it.\n",
        "\n",
        "A `tf.Tensor` is immutable. You can't change a tensor once it's created. It has a *value*, but no *state*. All the operations discussed so far are also stateless: The output of a `tf.matmul` only depends on its inputs.\n",
        "\n",
        "A `tf.Variable` has internal state, its value. When you use the variable, the state is read. It's normal to calculate a gradient with respect to a variable, but the variable's state blocks gradient calculations from going farther back. For example:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "C1tLeeRFE479"
      },
      "outputs": [

      ],
      "source": [

      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xKA92-dqF2r-"
      },
      "source": [
        "Similarly `tf.data.Dataset` iterators and `tf.queue`s are stateful, and will stop all gradients on tensors that pass through them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HHvcDGIbOj2I"
      },
      "source": [
        "## No gradient registered"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aoc-A6AxVqry"
      },
      "source": [
        "Some `tf.Operation`s are **registered as being non-differentiable** and will return `None`. Others have **no gradient registered**.\n",
        "\n",
        "The [tf.raw_ops](https://www.tensorflow.org/api_docs/python/tf/raw_ops) page shows which low-level ops have gradients registered.\n",
        "\n",
        "If you attempt to take a gradient through a float op that has no gradient registered the tape will throw an error instead of silently returning `None`. This way you know something has gone wrong.\n",
        "\n",
        "For example the `tf.image.adjust_contrast` function wraps [raw_ops.AdjustContrastv2](https://www.tensorflow.org/api_docs/python/tf/raw_ops#.AdjustContrastv2) which could have a gradient but the gradient is not implemented:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "HSb20FXc_V0U"
      },
      "outputs": [

      ],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pDoutjzATiEm"
      },
      "source": [
        "If you need to differentiate through this op you'll either need to implement the gradient and register it (using `tf.RegisterGradient`), or re-implement the function using other ops."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GCTwc_dQXp2W"
      },
      "source": [
        "## Zeros instead of None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TYDrVogA89eA"
      },
      "source": [
        "In some cases it would be convenient to get 0 instead of `None` for unconnected gradients.  You can decided what to return when you have unconnected gradients using the `unconnected_gradients` argument:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "U6zxk1sf9Ixx"
      },
      "outputs": [

      ],
      "source": [

      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Tce3stUlHN0L"
      ],
      "name": "autodiff.ipynb",
      "private_outputs": true,
      "provenance": [

      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
